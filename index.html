<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongchen Wei</title>
  
  <meta name="author" content="Hongchen Wei">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
	<link rel="icon" href="images/11.svg" type="image/png">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Hongchen Wei</name> 
                </p>
                <p>I am currently a second-year Ph.D. student in School of Remote Sensing and Information Engineering from Wuhan University, under the supervision of <a href="http://iip.whu.edu.cn/~zzchen/">Prof. Zhenzhong Chen</a>.
				<p>I received my M.E. degree in School of Computer Science and Engineering from Nanjing University of Science and Technology, China, in 2023.
				<p>I received my B.Sc. degree in School of Materials Science and Engineering from Xi'an Shiyou University, China, in 2020.
                <br>

                <p style="text-align:center">
                  <a href="mailto:hc_wei@whu.edu.cn">Email</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=CCQ3YsEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <!-- <a href="https://twitter.com/zhang_yuanhan">Twitter</a> &nbsp/&nbsp -->
                  <a href="https://github.com/hcwei13">Github</a> &nbsp/&nbsp
                  <a href="weihc_files\pdfs\CV_zh.pdf">CV</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/whc.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/whc.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
        </table>

		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Main Research Interests</heading>
            </td>
          </tr>
        </tbody></table>

        <ul>
          <li>Image/Video Captioning</li>
		  <li>Long Video Understanding</li>
		  <li>Spatial-Temporal Video Grounding</li>
		  <li>Large Multimodal Model</li>
        </ul>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <ul>
          <li>
            [2024-10] We propose the visual context window extension for long video understanding that enables the direct and easy scaling of pre-trained LMMs to 1024 frames, and significantly reducing memory usage.
          </li>
        </ul> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Pre-prints</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/frank-zero.gif' width="300" height="180">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2505.16151">
              <papertitle>Training-Free Reasoning and Reflection in MLLMs</papertitle>
            </a>
            <br>
            <!-- <strong>Hongchen Wei</strong>, Zhenzhong Chen -->
            <strong>Hongchen Wei</strong>, Zhenzhong Chen
            <br>
            <em>arXiv Preprint</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/longcaptioning.jpg' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2502.15393">
              <papertitle>LongCaptioning: Unlocking the Power of Long Caption Generation in Large Multimodal Models</papertitle>
            </a>
            <br>
            <!-- <strong>Hongchen Wei</strong>, Zhenzhong Chen -->
            <strong>Hongchen Wei</strong>, Zhihong Tan, Yaosi Hu, Chang Wen Chen, Zhenzhong Chen
            <br>
            <em>arXiv Preprint</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/lop.png' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://www.arxiv.org/abs/2506.12826">
              <papertitle>LOP: Learning Optimal Pruning for Efficient On-Demand MLLMs Scaling</papertitle>
            </a>
            <br>
            <!-- <strong>Hongchen Wei</strong>, Zhenzhong Chen -->
            Zhihan Zhang, Xiang Pan, <strong>Hongchen Wei</strong>, Zhenzhong Chen
            <br>
            <em>arXiv Preprint</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/rsfake.png' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2505.23283">
              <papertitle>RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries</papertitle>
            </a>
            <br>
            <!-- <strong>Hongchen Wei</strong>, Zhenzhong Chen -->
            Zhihong Tan, Jiayi Wang, Huiying Shi, Binyuan Huang, <strong>Hongchen Wei</strong>, Zhenzhong Chen
            <br>
            <em>arXiv Preprint</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/framework1.jpg' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://www.arxiv.org/abs/2409.20018">
              <papertitle>Visual Context Window Extension: A New Perspective for Long Video Understanding</papertitle>
            </a>
            <br>
            <strong>Hongchen Wei</strong>, Zhenzhong Chen
            <br>
            <em>ACM MM (CCF-A ‰ºöËÆÆ)</em>, 2025
            <br>
            <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a> 
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/realvg.png' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://hcwei13.github.io/">
              <papertitle>RealVG: Unleashing MLLMs for Training-Free Spatio-Temporal Video Grounding in the Wild</papertitle>
            </a>
            <br>
            <strong>Hongchen Wei</strong>, Zhenzhong Chen
            <br>
            <em>ACM MM (CCF-A ‰ºöËÆÆ)</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:45%;vertical-align:middle">
            <div class="one">
              <img src='images/RSSQA.jpg' width="320" height="140">
            </div>
          </td>
          <td style="padding:20px;width:55%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2502.13990">
              <papertitle>Remote Sensing Semantic Segmentation Quality Assessment based on Vision Language Model</papertitle>
            </a>
            <br>
            <!-- <strong>Hongchen Wei</strong>, Zhenzhong Chen -->
            Huiying Shi, Zhihong Tan, Zhihan Zhang, <strong>Hongchen Wei</strong>, Yaosi Hu, Yingxue Zhang, Zhenzhong Chen
            <br>
            <em>TGRS (CCF-B ÊúüÂàä)</em>, 2025
            <br>
            <!-- <a href='https://hcwei13.github.io/Visual-Context-Window-Extension/' target='_blank'>Project page</a>  -->
            <!-- <a href='https://huggingface.co/collections/lmms-lab/llava-next-video-661e86f5e8dabc3ff793c944' target='_blank'>Model</a> / -->
            <!-- <a href='https://github.com/LLaVA-VL/LLaVA-NeXT' target='_blank'>Code</a>  -->
            <!-- <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT?style=social">  -->
            <!-- <p>  This paper enhances long video understanding in Large Multimodal Models by extending visual context windows and introducing a progressive pooling strategy. </p> -->
          </td>
        </tr> 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/geneic.jpg' width="320" height="160">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2308.02862">
                <papertitle>Improving Generalization of Image Captioning with Unsupervised Prompt Learning</papertitle>
              </a>
              <br>
              <strong>Hongchen Wei</strong>, Zhenzhong Chen
              <br>
              <em>TOMM (CCF-B ÊúüÂàä)</em>, 2024
              <br>
              <!-- <a href="https://arxiv.org/abs/2203.07845">PDF</a> / 
              <a href="https://opengvlab.shlab.org.cn/bamboo/home">Project Page</a> / 
              <a href="https://huggingface.co/spaces/CVPR/Bamboo_ViT-B16_demo">Demo</a> /
              <a href="https://github.com/Davidzhangyuanhan/Bamboo">Code</a> 
              <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Davidzhangyuanhan/Bamboo?style=social"> 
              <p></p> -->
              
              <!-- <p> 4 times larger than ImageNet; 2 time larger than Object365; Built by active learning.</p> -->
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:45%;vertical-align:middle">
              <div class="one">
                <img src='images/tcyb.jpg' width="320" height="180">
              </div>
            </td>
            <td style="padding:20px;width:55%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9843903">
                <papertitle>Exploiting Cross-Modal Prediction and Relation Consistency for Semisupervised Image Captioning</papertitle>
              </a>
              <br>
			  Yang Yang, <strong> Hongchen Wei </strong>, Hengshu Zhu, Dianhai Yu, Hui Xiong, Jian Yang
              <br>
              <em>TCYB (CCF-B ÊúüÂàä)</em>, 2022 (Â≠¶Áîü‰∏Ä‰Ωú)
              <br>
              <a href="https://github.com/njustkmg/TCYB22_CPRC">Code</a> 
            </td>
          </tr> 

		  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

			<tr>
			  <td style="padding:20px;width:45%;vertical-align:middle">
				<div class="one">
				  <img src='images/tkdd.jpg' width="320" height="180">
				</div>
			  </td>
			  <td style="padding:20px;width:55%;vertical-align:middle">
				<a href="https://dl.acm.org/doi/abs/10.1145/3468675?sid=SCITRUS">
				  <papertitle>S2OSC: A Holistic Semi-Supervised Approach for Open Set Classification</papertitle>
				</a>
				<br>
				Yang Yang, <strong> Hongchen Wei </strong>, Zhenqiang Sun, Guangyu Li, Yuanchun Zhou, Hui Xiong, Jian Yang
				<br>
				<em>TKDD (CCF-B ÊúüÂàä)</em>, 2021 (Â≠¶Áîü‰∏Ä‰Ωú)
				<br>
				<!-- <a href="https://github.com/njustkmg/TCYB22_CPRC">Code</a>  -->
			  </td>
			</tr> 
          
        </tbody>
	</table>
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Activities</heading>
          </td>
          </tr>
        </tbody></table>

        <ul>
          <li>
            Reviewer: ICLR25, CVPR25, TNNLS
          </li>

          <!-- <li>
            Organizing Committee: <a href="https://theaitalks.org/">The AI Talk</a>, CVPR24 Workshop(<a hred="https://prompting-in-vision.github.io/index_cvpr24.html">Prompting in Vision</a>)
            ECCV22 Workshop(<a href="https://sense-human.github.io/">workshop1</a>, <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/">workshop2</a>)
          </li> -->
        </ul>


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Public Office Hour</heading>
          </td>
          </tr>
        </tbody></table> -->

        <!-- <ul>
          <li>
            <link href="https://assets.calendly.com/assets/external/widget.css" rel="stylesheet">
            <script src="https://assets.calendly.com/assets/external/widget.js" type="text/javascript" async></script>
            <a href="" onclick="Calendly.initPopupWidget({url: 'https://calendly.com/zhangyuanhan/15min'});return false;">Happy to chat about any topics :)</a>
          </li>
       </ul> -->




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated in Jun. 2025. </a>
              </p>
              <p style="text-align:right;font-size:small;">
                Homepage credits: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron.</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>